{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e826de68-227a-4f1a-aa32-1923c89bf2bb",
   "metadata": {},
   "source": [
    "# Комбинация модели Random Forest и байесовских методов\n",
    "Импорт необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db3b8486-c866-40a8-a18f-89fc50a394d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import pywt\n",
    "from scipy.stats import norm\n",
    "from scipy.signal import welch, spectrogram\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (classification_report, roc_auc_score, \n",
    "                            confusion_matrix, roc_curve, precision_recall_curve,\n",
    "                            average_precision_score)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline as make_imb_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98eaa0e-6918-4ee4-a488-761ab6f42883",
   "metadata": {},
   "source": [
    "# 1: Загрузка и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "962d5375-df5d-434b-918f-2e208ac9615a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка данных...\n",
      "Данные успешно загружены и преобразованы\n"
     ]
    }
   ],
   "source": [
    "print(\"Загрузка данных...\")\n",
    "df = pd.read_csv('final_classification_dataset.csv')\n",
    "\n",
    "def safe_convert(signal):\n",
    "    \"\"\"Безопасное преобразование сигнала в numpy array\"\"\"\n",
    "    try:\n",
    "        if isinstance(signal, str):\n",
    "            arr = np.array(ast.literal_eval(signal))\n",
    "        elif isinstance(signal, np.ndarray):\n",
    "            arr = signal\n",
    "        else:\n",
    "            arr = np.zeros(100)\n",
    "        \n",
    "        # Стандартизация длины\n",
    "        if len(arr) < 100:\n",
    "            arr = np.pad(arr, (0, 100 - len(arr)), 'constant')\n",
    "        return arr[:100]\n",
    "    except:\n",
    "        return np.zeros(100)\n",
    "\n",
    "# Преобразование сигналов\n",
    "df['clean_signal'] = df['clean_signal'].apply(safe_convert)\n",
    "df['noise_signal'] = df['noise_signal'].apply(safe_convert)\n",
    "print(\"Данные успешно загружены и преобразованы\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e20fa4-387a-43ce-9df0-77696ad897ae",
   "metadata": {},
   "source": [
    "# 2: Байесовский детектор сигналов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b32dc0c-1add-4b59-8d10-bedf76f29187",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianSignalDetector:\n",
    "    \"\"\"Класс для байесовского обнаружения сигналов\"\"\"\n",
    "    def __init__(self, noise_std=1.0, signal_std=0.5, prior_signal=0.5):\n",
    "        self.noise_std = noise_std\n",
    "        self.signal_std = signal_std\n",
    "        self.prior_signal = prior_signal\n",
    "        self.prior_noise = 1 - prior_signal\n",
    "        \n",
    "    def calculate_likelihood(self, x, hypothesis):\n",
    "        \"\"\"Вычисление правдоподобия\"\"\"\n",
    "        if hypothesis == 'signal':\n",
    "            return norm.pdf(x, 0, self.signal_std)\n",
    "        else:  # noise\n",
    "            return norm.pdf(x, 0, self.noise_std)\n",
    "    \n",
    "    def detect_signal(self, observations):\n",
    "        \"\"\"Обнаружение сигнала с вычислением апостериорных вероятностей\"\"\"\n",
    "        posteriors = []\n",
    "        for x in observations:\n",
    "            l_signal = self.calculate_likelihood(x, 'signal')\n",
    "            l_noise = self.calculate_likelihood(x, 'noise')\n",
    "            \n",
    "            p_signal = l_signal * self.prior_signal\n",
    "            p_noise = l_noise * self.prior_noise\n",
    "            \n",
    "            total = p_signal + p_noise\n",
    "            posteriors.append((p_signal/total, p_noise/total))\n",
    "        \n",
    "        return np.array(posteriors)\n",
    "\n",
    "class AdaptiveBayesianFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Адаптивный извлекатель байесовских признаков\"\"\"\n",
    "    def __init__(self, n_detectors=3):\n",
    "        self.n_detectors = n_detectors\n",
    "        self.detectors_ = []\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if y is None:\n",
    "            raise ValueError(\"Требуется y для обучения\")\n",
    "            \n",
    "        noise_signals = X.loc[y==0, 'noise_signal'] if 'noise_signal' in X.columns else X.iloc[:, 0]\n",
    "        signal_signals = X.loc[y==1, 'noise_signal'] if 'noise_signal' in X.columns else X.iloc[:, 0]\n",
    "        \n",
    "        noise_params = self._estimate_parameters(noise_signals)\n",
    "        signal_params = self._estimate_parameters(signal_signals)\n",
    "        \n",
    "        self.detectors_ = []\n",
    "        for i in range(self.n_detectors):\n",
    "            noise_std = noise_params[0] * (0.8 + i*0.2)\n",
    "            signal_std = signal_params[1] * (0.7 + i*0.3)\n",
    "            prior = 0.3 + i*0.2\n",
    "            \n",
    "            self.detectors_.append(\n",
    "                BayesianSignalDetector(noise_std=noise_std, \n",
    "                                    signal_std=signal_std,\n",
    "                                    prior_signal=prior)\n",
    "            )\n",
    "        return self\n",
    "    \n",
    "    def _estimate_parameters(self, signals):\n",
    "        \"\"\"Оценка параметров распределения\"\"\"\n",
    "        if isinstance(signals, pd.Series):\n",
    "            values = np.concatenate([s.flatten() for s in signals])\n",
    "        else:\n",
    "            values = signals.flatten()\n",
    "            \n",
    "        std = np.std(values)\n",
    "        return (std, std*0.7)\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Преобразование данных в байесовские признаки\"\"\"\n",
    "        features = []\n",
    "        for i, detector in enumerate(self.detectors_):\n",
    "            signals = X['noise_signal'] if 'noise_signal' in X.columns else X.iloc[:, 0]\n",
    "            \n",
    "            detector_features = signals.apply(\n",
    "                lambda x: self._extract_detector_features(x, detector, i))\n",
    "            features.append(pd.DataFrame(detector_features.tolist()))\n",
    "        \n",
    "        return pd.concat(features, axis=1)\n",
    "    \n",
    "    def _extract_detector_features(self, signal, detector, detector_idx):\n",
    "        \"\"\"Извлечение признаков из детектора\"\"\"\n",
    "        posteriors = detector.detect_signal(signal)\n",
    "        return {\n",
    "            f'bayes_{detector_idx}_p_signal_mean': np.mean(posteriors[:, 0]),\n",
    "            f'bayes_{detector_idx}_p_signal_std': np.std(posteriors[:, 0]),\n",
    "            f'bayes_{detector_idx}_p_signal_max': np.max(posteriors[:, 0]),\n",
    "            f'bayes_{detector_idx}_p_signal_min': np.min(posteriors[:, 0]),\n",
    "            f'bayes_{detector_idx}_snr_ratio': np.mean(posteriors[:, 0]) / (np.mean(posteriors[:, 1]) + 1e-10),\n",
    "            f'bayes_{detector_idx}_signal_ratio': np.sum(posteriors[:, 0] > 0.5) / len(signal)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe3208b-4236-41bd-b247-0eedf6ff7b56",
   "metadata": {},
   "source": [
    "# 3: Извлечение признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2f78ab0-4c8d-4d8e-8a2e-311fadc12f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Извлечение признаков...\n",
      "Извлечение байесовских признаков...\n",
      "Извлечение признаков завершено\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nИзвлечение признаков...\")\n",
    "\n",
    "def extract_enhanced_features(signal, fs=1000):\n",
    "    \"\"\"Извлечение временных, частотных и вейвлет-признаков\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # 1. Статистические признаки\n",
    "    features.update({\n",
    "        'mean': np.mean(signal),\n",
    "        'std': np.std(signal),\n",
    "        'max': np.max(signal),\n",
    "        'min': np.min(signal),\n",
    "        'median': np.median(signal),\n",
    "        'q25': np.percentile(signal, 25),\n",
    "        'q75': np.percentile(signal, 75),\n",
    "        'rms': np.sqrt(np.mean(signal**2)),\n",
    "        'zero_crossings': ((signal[:-1] * signal[1:]) < 0).sum(),\n",
    "        'entropy': np.sum(-signal * np.log(signal + 1e-10)),\n",
    "        'skewness': pd.Series(signal).skew(),\n",
    "        'kurtosis': pd.Series(signal).kurtosis()\n",
    "    })\n",
    "    \n",
    "    # 2. Частотные признаки\n",
    "    try:\n",
    "        f, Pxx = welch(signal, fs=fs, nperseg=min(len(signal), 256))\n",
    "        features.update({\n",
    "            'spectral_centroid': np.sum(f * Pxx) / np.sum(Pxx),\n",
    "            'spectral_bandwidth': np.sqrt(np.sum((f - features['spectral_centroid'])**2 * Pxx)),\n",
    "            'spectral_entropy': -np.sum(Pxx * np.log2(Pxx + 1e-12)),\n",
    "            'spectral_flatness': np.exp(np.mean(np.log(Pxx + 1e-10))) / np.mean(Pxx),\n",
    "            'spectral_energy': np.sum(Pxx),\n",
    "            'peak_frequency': f[np.argmax(Pxx)]\n",
    "        })\n",
    "    except:\n",
    "        features.update({k: 0 for k in ['spectral_centroid', 'spectral_bandwidth', \n",
    "                                      'spectral_entropy', 'spectral_flatness',\n",
    "                                      'spectral_energy', 'peak_frequency']})\n",
    "    \n",
    "    # 3. Вейвлет-признаки\n",
    "    try:\n",
    "        coeffs = pywt.wavedec(signal, 'db1', level=3)\n",
    "        for i, c in enumerate(coeffs):\n",
    "            features.update({\n",
    "                f'wavelet_l{i}_mean': np.mean(c),\n",
    "                f'wavelet_l{i}_std': np.std(c),\n",
    "                f'wavelet_l{i}_energy': np.sum(c**2),\n",
    "                f'wavelet_l{i}_entropy': -np.sum(c**2 * np.log(c**2 + 1e-10))\n",
    "            })\n",
    "        total_energy = np.sum([np.sum(c**2) for c in coeffs])\n",
    "        features.update({\n",
    "            'wavelet_total_energy': total_energy,\n",
    "            'wavelet_energy_ratio': np.sum(coeffs[0]**2) / (total_energy + 1e-10)\n",
    "        })\n",
    "    except:\n",
    "        wavelet_features = {f'wavelet_l{i}_{j}': 0 for i in range(4) \n",
    "                          for j in ['mean', 'std', 'energy', 'entropy']}\n",
    "        wavelet_features.update({\n",
    "            'wavelet_total_energy': 0,\n",
    "            'wavelet_energy_ratio': 0\n",
    "        })\n",
    "        features.update(wavelet_features)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_spectrogram_features(signal, fs=1000):\n",
    "    \"\"\"Извлечение признаков из спектрограммы\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        f, t, Sxx = spectrogram(signal, fs=fs, nperseg=256)\n",
    "        spectral_centroid = np.sum(f[:, None] * Sxx, axis=0) / np.sum(Sxx, axis=0)\n",
    "        features.update({\n",
    "            'spec_centroid_mean': np.mean(spectral_centroid),\n",
    "            'spec_centroid_std': np.std(spectral_centroid),\n",
    "            'spec_centroid_max': np.max(spectral_centroid),\n",
    "            'spec_centroid_min': np.min(spectral_centroid),\n",
    "            'spec_bandwidth_mean': np.mean(np.sqrt(np.sum((f[:, None] - spectral_centroid)**2 * Sxx, axis=0) / np.sum(Sxx, axis=0))),\n",
    "            'spec_entropy_mean': np.mean(-np.sum(Sxx * np.log(Sxx + 1e-10), axis=0)),\n",
    "            'spec_flatness_mean': np.mean(np.exp(np.mean(np.log(Sxx + 1e-10), axis=0)) / np.mean(Sxx, axis=0)),\n",
    "            'spec_total_energy': np.sum(Sxx),\n",
    "            'spec_peak_freq': f[np.argmax(np.mean(Sxx, axis=1))]\n",
    "        })\n",
    "    except:\n",
    "        features.update({\n",
    "            'spec_centroid_mean': 0, 'spec_centroid_std': 0,\n",
    "            'spec_centroid_max': 0, 'spec_centroid_min': 0,\n",
    "            'spec_bandwidth_mean': 0, 'spec_entropy_mean': 0,\n",
    "            'spec_flatness_mean': 0, 'spec_total_energy': 0,\n",
    "            'spec_peak_freq': 0\n",
    "        })\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Извлечение стандартных признаков\n",
    "clean_features = pd.DataFrame(df['clean_signal'].apply(\n",
    "    lambda x: {**extract_enhanced_features(x), **extract_spectrogram_features(x)}\n",
    ").tolist()).add_prefix('clean_')\n",
    "\n",
    "noise_features = pd.DataFrame(df['noise_signal'].apply(\n",
    "    lambda x: {**extract_enhanced_features(x), **extract_spectrogram_features(x)}\n",
    ").tolist()).add_prefix('noise_')\n",
    "\n",
    "# Подготовка данных для байесовских признаков\n",
    "bayes_df = df[['noise_signal']].copy()\n",
    "bayes_df['class'] = df['class']\n",
    "\n",
    "# Извлечение байесовских признаков\n",
    "print(\"Извлечение байесовских признаков...\")\n",
    "bayes_extractor = AdaptiveBayesianFeatures(n_detectors=3)\n",
    "bayes_features = bayes_extractor.fit_transform(bayes_df.drop('class', axis=1), bayes_df['class'])\n",
    "\n",
    "# Объединение всех признаков\n",
    "X = pd.concat([clean_features, noise_features, bayes_features,\n",
    "               df[['clean_signal_strength', 'noise_signal_strength']]], axis=1)\n",
    "y = df['class']\n",
    "\n",
    "# Обработка NaN/бесконечных значений\n",
    "X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "print(\"Извлечение признаков завершено\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13464fcd-be71-467d-860a-489a9fefc0a4",
   "metadata": {},
   "source": [
    "# 4: Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a6278-4c4e-4be1-bc52-d57c6389f862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Настройка конвейера обучения...\n",
      "\n",
      "Запуск GridSearchCV...\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nНастройка конвейера обучения...\")\n",
    "\n",
    "pipeline = make_imb_pipeline(\n",
    "    RobustScaler(),\n",
    "    SMOTE(random_state=42),\n",
    "    SelectKBest(score_func=f_classif),\n",
    "    RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'selectkbest__k': [30, 40, 50],\n",
    "    'randomforestclassifier__n_estimators': [300, 400],\n",
    "    'randomforestclassifier__max_depth': [None],\n",
    "    'randomforestclassifier__min_samples_split': [2, 5],\n",
    "    'randomforestclassifier__max_features': ['log2']\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"\\nЗапуск GridSearchCV...\")\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, \n",
    "                         scoring='roc_auc', n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"\\nЛучшие параметры:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25a5575-523a-4f14-96e7-5fb98b1a30ca",
   "metadata": {},
   "source": [
    "# 5: Оценка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000e9ca1-a087-448b-a0f7-eed0960753a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y):\n",
    "    \"\"\"Функция для оценки качества модели\"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    y_proba = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    print(\"\\nОтчет о классификации:\")\n",
    "    print(classification_report(y, y_pred, target_names=['Шум', 'Сигнал']))\n",
    "    \n",
    "    # Матрица ошибок\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Шум', 'Сигнал'],\n",
    "                yticklabels=['Шум', 'Сигнал'])\n",
    "    plt.title('Матрица ошибок')\n",
    "    plt.ylabel('Истинный класс')\n",
    "    plt.xlabel('Предсказанный класс')\n",
    "    plt.show()\n",
    "    \n",
    "    # ROC и PR кривые\n",
    "    fpr, tpr, _ = roc_curve(y, y_proba)\n",
    "    roc_auc = roc_auc_score(y, y_proba)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(fpr, tpr, label=f'ROC кривая (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('Ложноположительная частота')\n",
    "    plt.ylabel('Истинноположительная частота')\n",
    "    plt.title('ROC кривая')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y, y_proba)\n",
    "    ap_score = average_precision_score(y, y_proba)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(recall, precision, label=f'PR кривая (AP = {ap_score:.2f})')\n",
    "    plt.xlabel('Полнота')\n",
    "    plt.ylabel('Точность')\n",
    "    plt.title('Кривая точности-полноты')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Важность признаков\n",
    "    selected_features = X.columns[model.named_steps['selectkbest'].get_support()]\n",
    "    importances = model.named_steps['randomforestclassifier'].feature_importances_\n",
    "    top_features = pd.Series(importances, index=selected_features).sort_values(ascending=False)[:15]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=top_features.values, y=top_features.index)\n",
    "    plt.title('Топ 15 важных признаков')\n",
    "    plt.xlabel('Важность')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    " \n",
    "print(\"\\nОценка лучшей модели...\")\n",
    "evaluate_model(best_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56004332-f1b6-4605-a1ab-fed8f8ec0193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
